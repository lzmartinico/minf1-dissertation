%% Evaluation
\section{Testing}
Throughout implementation, particular care was taken to follow software engineering best practices. Because JavaScript is a dynamically typed language, we lack a compiler's check for code correctness. As a consequence, we have to run code to find out if it works. To avoid deploying a broken commit to the server, we wrote a git pre-commit hook to test if the program doesn't crash immediately, and to pass a linter to catch any syntax error in the code:
\begin{lstlisting}
npm start > /dev/null &
sleep 5
if ``eslint *.js`` && [[ -n `pidof -k node` ]] ; then
    echo "Pass linter and npm doesn't crash"
    exit 0
else
    pkill node
    exit 1
fi
\end{lstlisting}
While this was a good tool to statically catch errors, some bugs would only emerge through dynamic testing. While the testmybot unit test library looked like a promising solution for establishing a routine of test-driven development, it soon was evident that the Facebook hooks are still not mature enough to be used in production, so we had to resort to manually testing each new feature.
\section{Evaluation}
For our evaluation, we ran an experiment giving out the chatbot to 10 university students, all within ages of 20 to 25 and at least moderatively physically active, to use for a week. As a control group, another 10 university students were prescribed to use the MyFitnessPal app for the same duration. All users were recruited through Facebook chat or in person, and all were given the OK to start the evaluation on the same day. In retrospect, having a  more gradual rollout might have helped with spotting the first bugs initially and giving us a chance to fix the underlying issues without compromising the platform for every other user.
\subsection{Record keeping}
Since this was our first usage of the chatbot outside our own testing, we expected to encounter a variety of bugs and phrasings that it had never encounter from us. We set up explicit logging function for all error case, printing the user id as well so as to be able to reconstruct the causes at a later stage. We could also access a complete record of all communication through the Facebook app console, as well as having a list of intents identified and how the parameters were matched from the Dialogflow agent. While having this much access gave us some great insight into what might be affecting faulty behaviours, it was also concerning how we could read the conversations in their entirety, and while Dialogflow allows to deactivate the logging, there was no way of doing that through Facebook. And even if there was, it would be trivially easy to still log everything through the server.



\subsection{Experiment description - survey, in person interview}
To initiate the experiment, users' Facebook profiles were added as testers through the Facebook developer console. They were then sent a link to the chatbot's Facebook page, where they could press a clearly visible button to start chatting. This would open a chat window, where they had the option of pressing a button to get started before being taken through their first conversation.
During user evaluation, features were pointed out: clear record if incorrect food, add food on different date
