As with all software artefacts that have a user facing component, testing can be lead both on a technical level and on a usability level. Since the chatbot infrastructure contains many different components, it will be necessary to conduct some testing on the robustness of each.
\section{Testing}
Throughout the whole implementation, particular care was taken to follow software engineering best practices. Because JavaScript is a dynamically typed language, we lack a compiler's check for code correctness. As a consequence, we have to run code to find out if it works. To avoid deploying a broken commit to the server, we wrote a git pre-commit hook to test if the program crashes immediately, and to pass a linter to catch any syntax error in the code:
\begin{lstlisting}
npm start > /dev/null &
sleep 5
if ``eslint *.js`` && [[ -n `pidof -k node` ]] ; then
    echo "Pass linter and npm doesn't crash"
    exit 0
else
    pkill node
    exit 1
fi
\end{lstlisting}
While the combination of linting and running the program was a good tool to statically catch some errors, some bugs we encountered during development could have been avoided if we had a safer type system. It is possible to add type annotation onto Javascript code using the Typescript language \cite{typescript}; additionally, there are Dialogflow client APIs for a variety of additional programming languages besides Node. It would even have been possible to use any other programming language, because the use of the client is not required (and our own implementation only took advantage of a minimal number of services it offers): the only requirement to handle fulfilment is a webhook that accepts the correct kind of HTTP requests and provides a correct response in return. 

It is also necessary to conduct some dynamic testing to ensure the interaction of all components is successful. The ecosystem for end to end bot testing is still very primitive, with some bot platforms offering their own bespoke testing utilities, and the only cross platform library being \textit{Botium}\footnote{https://github.com/codeforequity-at/botium-core} and its unit test frontend \textit{testmybot} \footnote{https://github.com/codeforequity-at/testmybot}. These library are inspired and offer the semantics of popular general purpose behaviour-driven testing frameworks, allowing for easily composing test cases through a simple syntax. \\
For example, one of our unit tests was simply a file containing the script we would expect from interacting with our chatbot:
\begin{lstlisting}
Begin Conversation Test Case

#me
greetings

#bot
Greetings! What's your name?

#me
You can call me Lorenzo

#bot
Hi Lorenzo pleasure to meet you!
I am Healthbot. I will be your personal diet assistant
You can tell me what you are eating and how much, or send me a picture of your food, and I will record it so we can try to understand how you eat better!
After that, whenever you want to think back about what you have been eating, just ask me to tell you what you had on any date!
\end{lstlisting}
While \textit{testbot} looked like a promising solution for establishing a routine of test-driven development, it soon was evident that the \textit{Botium} hooks for Facebook were still not mature enough to be used in production. \\
Therefore, rather than having a collection of sample conversations we could feed \textit{testmybot} to determine whether any new code change would break any of the responses we had been getting before, we had to resort to manually testing each new feature, by messaging the chatbot from a personal Facebook account, repeating the same script for each different functionality we had previously implemented. This would show very clearly if our modifications had broken an existing behaviour. In case of errors, most debugging information was printed to the Heroku server logs through the \textit{console.log} JavaScript function.

Various analytics tools exist for chatbot usage, both built-in in the chatbot platforms (both Dialogflow and Facebook Messenger provide one, although the first is still in beta), and external, like \textit{dashbot}\footnote{dashbot.io}. While we considered instrumenting Healthbot with one of these, the ecosystem did not seem mature enough, and the limited scope of our prototype made manual examination sufficient.

During the evaluation phase, we used Dialogflow's training module to update the intents with additional sample utterances we observed from our test users. While this helped us to refine some entities to include expressions we had missed, the training module did not seem to significantly improve accuracy; it remains to be seen whether it will be more useful once it comes out of beta.
\section{User trial}
For our evaluation, we ran an experiment giving out the chatbot to 11 university students, all within ages of 20 to 25 and at least moderately physically active, to use for a week. As a control group, another 9 university students were prescribed to use the MyFitnessPal app for the same duration. All users were recruited through Facebook chat or in person, and all were given the OK to start the evaluation on the same day after having read and signed a consent form describing the experiment and the tester's role in it. \\
In retrospect, having a more gradual rollout might have helped with spotting the first bugs sooner, and giving us a chance to fix the underlying issues without compromising the platform for every other user. As it was, while we identified several issues and features that would have been immediately easy to add, we did not push most of the modification to avoid breaking existing users' workflows. Unfortunately Facebook does not allow to have a separate testing and production environments until the application goes through a first review process, which we could not afford to spend time going through.
\subsection{Record keeping}
Since this was our first usage of the chatbot outside our own testing, we expected to encounter a variety of bugs and phrasings that it had never encountered from us. We set up a detailed logging function for all error case, printing the user ID as well so as to be able to reconstruct the causes at a later stage. We could also access a complete record of all communication through the Facebook app console, as well as having a list of intents identified and how the parameters were matched from the Dialogflow agent. Although it could have been possible to keep a larger number of logs, for example for success cases, we thought it would make records illegible in the eventuality of having to go through debugging.

While having this much access gave us some great insight into what might be affecting faulty behaviours, it was very concerning how we could read the conversations in their entirety, and while Dialogflow allows to deactivate the logging, there was no way of doing that through Facebook. Even if there was, it would be trivially easy to still log everything through the server.
\subsection{Methodology}
To initiate the experiment, the chatbot users' Facebook profiles were added as testers through the Facebook developer console. They were then sent a link to the Healthbot's Facebook page, where they could press a clearly visible button to start chatting. This would open a chat window, where they had the option of pressing a button to get started before being taken through their first conversation. Users were given no indication on how to precede, except for the chatbot's introductory message. Over the course of the evaluation, users sent us some questions (never through the chatbot) on what they could do with it. 

The MyFitnessPal testers were asked to give feedback a week after the evaluation started; the bot testers were sent feedback forms after 9 days.\\
The surveys sent to both testers were built using the Google Forms online tool. Most questions were similar to both questionnaires, with some variation when it came to input methods and displays dependent on the app. In compliance with the consent form, none of the questions were made compulsory, and the survey was made anonymous.

The questionnaire asked some background information on the participant, to establish levels of fitness and computer literacy, thoughts on nutrition and previous dietary and food tracking histories.\\
Testers were then asked their opinion on the usability, utility, pleasantness of the entire platform they were evaluating, as well as for each specific functionality, and if they had any feedback on things they would have liked to see. Some answers were multiple choices, checkboxes or Likert scales, but most were open text input to allow the participant to give a full explanation of the reasoning behind their answers.\\
The full survey and responses can be found in the Appendix.
\subsection{Response}
7 participants responded to the MyFitnessPal survey, and 9 to the chatbot survey. While some questions were answered by all the participants who took the survey, none of the open ended questions were answered by all, with some questions or even entire sections were ignored by more than half the respondents. 

Participants seemed to be distributed similarly across the two trials, with chatbot users being slightly more proficient with computers, as well as being more aware of their fitness levels. Similar splits were evident in the proportions of participants who had dieted before, with around three quarters of participants citing a good current health or scepticism with established diet, and some chatbot users using laziness as a motivation. The minority of users who had dieted chose to do so because of environmental, athletic or health-related issues, but did not maintain dieting after reaching their goals, or because of commitment issues. \\
Among both groups, about half the participants consistently had 3 meals per day, with some having a variable number of meals and no participants consuming less than two; our chatbot users however in general snacked less than MyFitnessPal users (there is a possibility that these answers might have been influenced by the experiment, even if participants were encouraged to think about their behaviour before; the fact that MyFitnessPal presents snack as a distinctly separate category, and the chatbot doesn't, might have affected responses to this question). \\
About half of the participants reported having tracked their diet before, either keeping a food diary, memorising their meal, or, the majority of respondents, using MyFitnessPal, and most previous trackers also kept a record of their snacks. 

For unclear reasons, more than half the respondents skipped the section about their dietary makeup, but for those who did fill it in, definitions of ``balanced diet'' varied significantly: while a majority named a variation of having a correct proportion of Protein, Carbohydrates and Fats, with some allowing for vitamins and minerals as well, others named calories as a main concern, reducing some unhealthy food groups and increasing others, or avoiding stressing about their diet and making sure to have what made them feel good. Only half of the respondents consider their diet to be balanced, including all those who planned their meals in advanced, and most respondents tend to cook their own meals, eat out or do both things in equal measure.

MyFitnessPal user found the app on average more useful than users of the chatbot, although the latter was rated as generally more pleasant to use. The food diary, the macronutrient breakdown graph and the remaining calorie counter were all generally considered clear and useful, with the graph being the most pleasant feedback. For input, the majority of users preferred scanning the barcode of the meal they were having, although for some the kind of food they were eating was a factor, and people who tended to cook their own food preferred text entries. All users had some issues with finding the food they wanted using text entry, but no one complained about the method being too slow; barcode scanning seemed to perform better, with only some users reporting difficulties identifying a barcode or matching the correct item in the database. \\
By contrast, chatbot users generally found the feedback useless, or insufficient. Chatting was highly preferred as an input method, although several participants did not think it understood their queries well enough, and some were annoyed by the prompts for size. Some users who took pictures for input found it was not accurate enough, but the larger problem of this feature seemed to be people who were not aware of the functionality.

Retention rates were much higher for MyFitnessPal users, with the largest missed meals estimated to be 5, and some user logging all their meals; chatbot users, instead, were much less active, with only one person logging almost every meal, and everyone else estimating having missed between 5 and 20. For both populations, the leading cause of missing a meal log was lack of time or forgetfulness, with some chatbot users finding input methods cumbersome or lack of interest because the feedback did not seem useful. \\
As a consequence, almost all chatbot testers did not log their meals on several days. Only half of the users reported having received a reminder the day after (which might be explainable by the fact that, while everyone received a reminder, some users had been active the day before). The reminders were generally found to be useful, and mostly made the users log their food on the day, and one user even expressed a desire to receive more frequent prompts to avoid forgetting meals. \\
MyFitnessPal also provided a reminder functionality, but it is off by default. All users who turned it on got a reminder, but it did not convince them use the app after. 

One stark difference in response between chatbot and app user was on preference between noting their food records with absolute measurement (number of portions or unit of measurement plus numerical value). MyFitnessPal users overwhelmingly declared a preference for absolute value metrics, because of the need to calculate precise calorie counts that the app provides, and as a more reliable comparison method to standard recommended portion sizes. The majority of chatbot users instead indicated a preference for relative values, because it is easier not to have to constantly measure portions. 

Despite the fact that the utility of a food diary comes from the ability to look back on previous meals, only a third of the chatbot users, and just over half the app users took advantage of this feature on a later date, and those who did reported the information presented to them to be accurate, but unhelpful; in fact, about half of the chatbot users and two thirds of the app user do not think using the meal log has given them a better idea of how they eat. 

Overall, most participants did not think that logging their food had helped them to eat better, although for many users that was because they already are happy with their diet. Those that registered a positive impact mentioned that having a better oversight on their food trends did prove helpful for them, and MyFitnessPal user specified sugar tracking and suggested recipes as useful features, although some comments also pointed out that the paid version of the app could have been more useful. However, two thirds of chatbot users found that they had become more ``mindful'' about their diet by using the chatbot, as opposed to less than half of the MyFitnessPal users. 

Expectations for the chatbot were high for some users who were hoping for ``\textit{[a] good AI}'' which would be talkative and give them active reminders and regular feedback; some were just looking for a more convenient way to log their food; but most participants did not expect much from it. Needless to say, the former group were disappointed by our implementation, with the natural language parsing of quantities, repetitive replies and image recognition capabilities being particularly frustrating. At the same time, users appreciated the ability to choose input method, and some found the chatbot's personality less annoying than they expected. MyFitnessPal tester also were expecting ease of use, a complete database, and a tool that would prompt small change in their behaviours by highlighting trends that were needed to be changed. Most of these were met by participants, although the majority of American commercial products in the database was deemed a problem. 

When asked if they would continue to log their meals after the evaluation period, participants on both platforms were mostly uninterested, either because they did not find it useful enough, or because logging took too much time, and in the case of the chatbot, they perceived the product development as not being ready enough for regular usage. However, some users who seemed to have benefited from its usage were willing to continue using, or at least consider it in case of future more rigorous dieting, and one MyFitnessPal user was convinced to resume their paper food diary. 

About half the MyFitnessPal users enabled fitness tracking functions, which seemed generally well received, although there were some concerns to how accurate their estimations were, and how useful it is to simply subtract exercise from calorie intake from a nutritional standpoint. Participants who did not use the feature were potentially interested, but the interface was not easy enough to understand, and there were perceived barriers to entry such as downloading a separate companion app or owning a smartwatch to better track calorie expenditure. A few chatbot users tried texting about their activity, but when they got no reply they did not make another attempt.

Testers of the app suggested they would have liked to have dedicated fruit and vegetable counters, automatic exercise calorie calculations and personalized recipe suggestions based on a specific ingredient or past meals and goals. For the chatbot, suggestions included pointing out a food's recommended amount, more reminders, especially around 5-a-day tracking, retroactively adding past meals, adding more variation to the automatic replies to make them less boring, and better onboarding functionality. \\
As part of the survey participants were  also asked if they thought that the information they were uploading was being kept safe, and if they thought it was an important concern. Most participants were actually concerned about their dietary records being exposed, with some particularly concerned with being judged because of their diet, while others did not think food records were a particularly sensitive topic, and anonymising dietary data could be used to benefit medical research organisations. Users of the chatbot generally considered their information to be secured, and while one participant specified ``I know its developer takes security seriously'', another identified that platform issues were a problem: ``I mean it's on facebook so not really.'' \footnote{A very cognisant assesment, in retrospect, given the revelations transpired through the Cambridge Analytica scandal \cite{cambridgeanalytica}}. On the other hand, MyFitnessPal users were more concerned or unsure whether their information was safe or not, and with good reason: two days after the study completed, the app's parent company \textit{Under Armour} publicly announced it had been a victim in one of the largest ever leaks of user personal information \cite{underarmour}.
